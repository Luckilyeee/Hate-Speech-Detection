{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"pytorch","language":"python","name":"pytorch"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"W&F_T_torch.ipynb","provenance":[{"file_id":"1ReiXYQyPm0aTF389Wfwjzw2S6Wso0uJN","timestamp":1611355788473},{"file_id":"1xXFoOCmoHI1IFpOpBp9JlW_XiejVCq-w","timestamp":1611173506530}],"collapsed_sections":[],"toc_visible":true},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"Ffx6_fqCEy6F"},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","\n","import torchvision\n","import torch.optim as optim\n","\n","import numpy as np\n","from sklearn.manifold import TSNE\n","\n","import argparse, sys, os\n","\n","import torch\n","from torchtext import data, datasets\n","import random\n","torch.backends.cudnn.deterministic = True\n","\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.autograd import Variable\n","import torch.nn.init as init\n","from torch.utils.data import DataLoader\n","from torchvision import datasets, transforms\n","\n","import time\n","from collections import Counter\n","import matplotlib.pyplot as plt \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"resources":{"http://localhost:8080/nbextensions/google.colab/files.js":{"data":"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","ok":true,"headers":[["content-type","application/javascript"]],"status":200,"status_text":""}},"base_uri":"https://localhost:8080/","height":72},"id":"LgTW5dFNE2H_","executionInfo":{"status":"ok","timestamp":1615231637465,"user_tz":420,"elapsed":20239,"user":{"displayName":"Peiyu Li","photoUrl":"","userId":"12219071892359382680"}},"outputId":"75f96dfd-ca95-411a-dfd6-b271a91dbb92"},"source":["from google.colab import files\n","uploaded = files.upload()"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["\n","     <input type=\"file\" id=\"files-3f866c4e-13c2-41c8-8ffc-06dbca9025c8\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-3f866c4e-13c2-41c8-8ffc-06dbca9025c8\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script src=\"/nbextensions/google.colab/files.js\"></script> "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Saving Wiki.csv to Wiki.csv\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ktf9iHSq_mME","executionInfo":{"status":"ok","timestamp":1615231641594,"user_tz":420,"elapsed":1050,"user":{"displayName":"Peiyu Li","photoUrl":"","userId":"12219071892359382680"}},"outputId":"0058ad5f-fe64-4d4b-a2af-0f75db31340c"},"source":["import pandas as pd\n","\n","import nltk\n","import ssl\n","\n","try:\n","    _create_unverified_https_context = ssl._create_unverified_context\n","except AttributeError:\n","    pass\n","else:\n","    ssl._create_default_https_context = _create_unverified_https_context\n","nltk.download('stopwords')\n","\n","\n","twitter = pd.read_csv(\"twitter.csv\")\n","forum = pd.read_csv(\"Forum.csv\")\n","wiki = pd.read_csv(\"Wiki.csv\")"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"_gWis_-FFelK"},"source":["# remove puctunations, https\n","import re\n","def  clean_text(df, text_field):\n","    df[text_field] = df[text_field].str.lower()\n","    df[text_field] = df[text_field].apply(lambda elem: re.sub(r\"(@[A-Za-z]+)|([^A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", elem))\n","    return df\n","\n","twitter = clean_text(twitter, 'text')\n","forum = clean_text(forum, 'text')\n","wiki = clean_text(wiki, 'text')\n","\n","stopwords = nltk.corpus.stopwords.words('english')\n","twitter['text'] = twitter['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n","forum['text'] = forum['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n","wiki['text'] = wiki['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e3JqJqytHq7I"},"source":["twitter['domain'] = 1\n","forum['domain'] = 0\n","wiki['domain'] = 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dYTRjR1SEPe5"},"source":["twitter = twitter[['text', 'label', 'domain']]\n","forum = forum[['text', 'label', 'domain']]\n","wiki = wiki[['text', 'label', 'domain']]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iELDQsH8kmTQ"},"source":["forum.to_csv('F.csv', index=False)\n","twitter.to_csv('T.csv', index=False)\n","wiki.to_csv('W.csv', index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"anrJjNazm2Dr"},"source":["#tokenize"]},{"cell_type":"code","metadata":{"id":"dTMn9hlpHCPC"},"source":["SEED = 1234\n","torch.manual_seed(SEED)\n","torch.backends.cudnn.deterministic = True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLuRpTKRgwGd","colab":{"base_uri":"https://localhost:8080/","height":198},"executionInfo":{"status":"error","timestamp":1615231923408,"user_tz":420,"elapsed":350,"user":{"displayName":"Peiyu Li","photoUrl":"","userId":"12219071892359382680"}},"outputId":"30571c6f-b133-4b69-9916-7fdfa987a114"},"source":["TEXT = data.Field(tokenize = 'spacy', batch_first=True)\n","LABEL = data.LabelField(dtype=torch.float)\n","DOMAIN = data.LabelField(dtype=torch.float)"],"execution_count":null,"outputs":[{"output_type":"error","ename":"AttributeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-cfd36155b06c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mTEXT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'spacy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_first\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mLABEL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mDOMAIN\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLabelField\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: module 'torchtext.data' has no attribute 'Field'"]}]},{"cell_type":"code","metadata":{"id":"NZ0GfBoYhD-i"},"source":["#loading custom dataset\n","T =data.TabularDataset(path = 'T.csv',format = 'csv',fields = [('text', TEXT),('label',LABEL),('domain',DOMAIN)],skip_header = True)\n","W =data.TabularDataset(path = 'W.csv',format = 'csv',fields = [('text', TEXT),('label',LABEL),('domain',DOMAIN)],skip_header = True)\n","forum =data.TabularDataset(path = 'F.csv',format = 'csv',fields = [('text', TEXT),('label',LABEL),('domain',DOMAIN)],skip_header = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Es5xDO61mmcr"},"source":["#print preprocessed text\n","print(vars(forum.examples[0]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PR9unV5K1NCP"},"source":["split train, test, val"]},{"cell_type":"code","metadata":{"id":"WgVrdNMZnvYh"},"source":["def split(train_size, dataset):\n","  train, test = dataset.split([6000, 2000], random_state = random.seed(SEED))\n","  train, other = train.split([train_size, 6000-train_size], random_state = random.seed(SEED))\n","  \n","  return train, test\n","\n","train_size = 2000\n","T_train_data, T_test_data = split(train_size, T)\n","F_train_data, F_test_data = split(train_size, forum)\n","W_train_data, W_test_data = split(train_size, W)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gdjUy2Etm6v2"},"source":["#convert texts into integer sequences."]},{"cell_type":"markdown","metadata":{"id":"b7i3AD6x1STl"},"source":["build vocabulary"]},{"cell_type":"code","metadata":{"id":"kk7nXdz5nOVk"},"source":["#initialize glove embeddings\n","MAX_VOCAB_SIZE = 25_000\n","\n","TEXT.build_vocab(T_train_data, F_train_data, W_train_data, \n","                 max_size = MAX_VOCAB_SIZE, \n","                 vectors = \"glove.6B.100d\", \n","                 unk_init = torch.Tensor.normal_) \n","LABEL.build_vocab(T_train_data, F_train_data, W_train_data)\n","DOMAIN.build_vocab(T_train_data, F_train_data, W_train_data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WDerTCvs1WOQ"},"source":["creat iterators"]},{"cell_type":"code","metadata":{"id":"Zj8cuxRKqf9T"},"source":["#set batch size\n","BATCH_SIZE = 64\n","\n","#check whether cuda is available\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n","\n","#Load an iterator\n","T_train_iterator, T_test_iterator = data.BucketIterator.splits(\n","    (T_train_data,  T_test_data), \n","    batch_size = BATCH_SIZE, \n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True,\n","    device = device)\n","\n","F_train_iterator, F_test_iterator = data.BucketIterator.splits(\n","    (F_train_data, F_test_data), \n","    batch_size = BATCH_SIZE, \n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True,\n","    device = device)\n","\n","W_train_iterator, W_test_iterator = data.BucketIterator.splits(\n","    (W_train_data, W_test_data), \n","    batch_size = BATCH_SIZE, \n","    sort_key = lambda x: len(x.text),\n","    sort_within_batch=True,\n","    device = device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c9B6b7IkOZTX"},"source":["print(len(T_train_iterator))\n","print(len(T_test_iterator))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3fWSEplb4vh0"},"source":["print(len(T_train_data))\n","print(len(T_test_data))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OTJazV8KU0O2"},"source":["\n","class CNN1d(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n","                 dropout, pad_idx):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        \n","        self.convs = nn.ModuleList([\n","                                    nn.Conv1d(in_channels = embedding_dim, \n","                                              out_channels = n_filters, \n","                                              kernel_size = fs)\n","                                    for fs in filter_sizes\n","                                    ])\n","        \n","        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","        \n","    def forward(self, text):\n","        \n","        #text = [batch size, sent len]\n","        \n","        embedded = self.embedding(text)\n","                \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        embedded = embedded.permute(0, 2, 1)\n","        \n","        #embedded = [batch size, emb dim, sent len]\n","        \n","        conved = [F.relu(conv(embedded)) for conv in self.convs]\n","            \n","        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","        \n","        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n","        \n","        #pooled_n = [batch size, n_filters]\n","        \n","        cat = self.dropout(torch.cat(pooled, dim = 1))\n","        \n","        #cat = [batch size, n_filters * len(filter_sizes)]\n","            \n","        return self.fc(cat)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XS3XkZz34-fo"},"source":["class GradReverse(torch.autograd.Function):\n","    \"\"\"\n","    Extension of grad reverse layer\n","    \"\"\"\n","    @staticmethod\n","    def forward(ctx, x, constant):\n","        ctx.constant = constant\n","        return x.view_as(x)\n","\n","    @staticmethod\n","    def backward(ctx, grad_output):\n","        grad_output = grad_output.neg() * ctx.constant\n","        return grad_output, None\n","\n","    def grad_reverse(x, constant):\n","        return GradReverse.apply(x, constant)\n","\n","class Extractor(nn.Module):\n","    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n","                 dropout, pad_idx, pretrained_embeddings):\n","        \n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n","        self.embedding.weight.data.copy_(pretrained_embeddings)\n","        self.conv_0 = nn.Conv1d(in_channels = 1, \n","                                out_channels = n_filters, \n","                                kernel_size = (filter_sizes, embedding_dim))\n","        \n","        \n","        \n","        self.fc = nn.Linear(n_filters, output_dim)\n","        \n","        self.dropout = nn.Dropout(dropout)\n","\n","        # self.relu = nn.ReLU(inplace=True)\n","        # self.max_pool1d = nn.MaxPool1d(2,2)\n","        \n","    def forward(self, text):\n","                \n","        #text = [batch size, sent len]\n","        \n","        embedded = self.embedding(text)\n","                \n","        #embedded = [batch size, sent len, emb dim]\n","        \n","        embedded = embedded.unsqueeze(1)\n","        \n","        #embedded = [batch size, 1, sent len, emb dim]\n","        \n","        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n","            \n","        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n","        \n","        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n","        \n","        #pooled_n = [batch size, n_filters]\n","        x = self.dropout(pooled_0)\n","        #x = [batch size, n_filters * len(filter_sizes)]\n","            \n","        return x\n","\n","class Class_classifier(nn.Module):\n","\n","    def __init__(self, n_filters, output_dim):\n","        super(Class_classifier, self).__init__()\n","        self.fc = nn.Linear(n_filters, output_dim)\n","\n","    def forward(self, input):\n","        x = self.fc(input)\n","        return x\n","\n","class Domain_classifier(nn.Module):\n","\n","    def __init__(self, n_filters, output_dim):\n","        super(Domain_classifier, self).__init__()\n","        self.fc = nn.Linear(n_filters, output_dim)\n","\n","    def forward(self, input, constant):\n","        input = GradReverse.grad_reverse(input, constant)\n","        return self.fc(input)\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Wt1-nKmNEy6L"},"source":["def optimizer_scheduler(optimizer, p):\n","    \"\"\"\n","    Adjust the learning rate of optimizer\n","    :param optimizer: optimizer for updating parameters\n","    :param p: a variable for adjusting learning rate\n","    :return: optimizer\n","    \"\"\"\n","    for param_group in optimizer.param_groups:\n","        param_group['lr'] = 0.01 / (1. + 10 * p) ** 0.75\n","\n","    return optimizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Mvcv_kj1k5z8"},"source":["INPUT_DIM = len(TEXT.vocab)\n","EMBEDDING_DIM = 100\n","N_FILTERS = 100\n","FILTER_SIZES = 1\n","OUTPUT_DIM = 1\n","DROPOUT = 0.5\n","PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n","pretrained_embeddings = TEXT.vocab.vectors\n","\n","extractor = Extractor(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX,pretrained_embeddings )\n","class_classifier = Class_classifier(N_FILTERS, OUTPUT_DIM)\n","domain_classifier = Domain_classifier(N_FILTERS, OUTPUT_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lfAtWJbz2uWd"},"source":["check the number of parameters"]},{"cell_type":"code","metadata":{"id":"NzXB4OXg2vcp"},"source":["def count_parameters(model):\n","    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n","\n","print(f'The model has {count_parameters(extractor) + count_parameters(class_classifier) + count_parameters(domain_classifier):,} trainable parameters')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fd1jGm-03LR1"},"source":["zero the initial weights of the unknown and padding tokens\n","\n","\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"Nr2wU_OS-m2d"},"source":["UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n","\n","extractor.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n","extractor.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9WfBc_FC3QxS"},"source":[""]},{"cell_type":"code","metadata":{"id":"1CN3A1sa91C8"},"source":["criterion = nn.BCEWithLogitsLoss()\n","\n","extractor = extractor.to(device)\n","class_classifier = class_classifier.to(device)\n","domain_classifier = domain_classifier.to(device)\n","\n","criterion = criterion.to(device)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TBJS8wtL3lnY"},"source":["define function to calculate the correct predictions"]},{"cell_type":"code","metadata":{"id":"a1TOd3znEy6M"},"source":["def train(training_mode, extractor, class_classifier, domain_classifier, criterion,\n","          source_data1, source_data2, target_data, optimizer, epoch):\n","  \n","    epoch_loss = 0\n","    epoch_acc = 0\n","\n","    # setup models\n","    extractor.train()\n","    class_classifier.train()\n","    domain_classifier.train()\n","\n","    # steps\n","    start_steps = epoch * len(source_data1)\n","    total_steps = 10 * len(source_data1)\n","\n","    for batch_idx,(sdata1, sdata2, tdata) in enumerate(zip(source_data1, source_data2, target_data)):\n","        if training_mode == 'dann':\n","            # setup hyperparameters\n","            p = float(batch_idx + start_steps) / total_steps\n","            constant = 2. / (1. + np.exp(-gamma * p)) - 1\n","\n","            # prepare the data\n","            input1, label1, domain1 = sdata1.text, sdata1.label, sdata1.domain\n","            input2, label2, domain2 = sdata2.text, sdata2.label, sdata2.domain\n","            input3, label3, domain3 = tdata.text, tdata.label, tdata.domain\n","    \n","            # setup optimizer\n","            optimizer = optimizer_scheduler(optimizer, p)\n","            optimizer.zero_grad()\n","\n","            # compute the output of source domain and target domain\n","            src_feature1 = extractor(input1)\n","            src_feature2 = extractor(input2)\n","            tgt_feature = extractor(input3)\n","\n","            # compute the class loss of src_feature\n","            class_pred1 = class_classifier(src_feature1).squeeze(1)\n","            class_pred2 = class_classifier(src_feature2).squeeze(1)\n","            \n","            class_loss1 = criterion(class_pred1, label1)\n","            class_loss2 = criterion(class_pred2, label2)\n","\n","            # compute the domain loss of src_feature and target_feature\n","            tgt_preds = domain_classifier(tgt_feature, constant).squeeze(1)\n","            src_pred1 = domain_classifier(src_feature1, constant).squeeze(1)\n","            src_pred2 = domain_classifier(src_feature2, constant).squeeze(1)\n","            \n","            tgt_loss = criterion(tgt_preds, domain3)\n","            src_loss1 = criterion(src_pred1, domain1)\n","            src_loss2 = criterion(src_pred2, domain2)\n","            \n","            domain_loss = tgt_loss + src_loss1 + src_loss2\n","            class_loss = class_loss1 + class_loss2\n","\n","            loss = class_loss + theta * domain_loss\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            # print loss\n","            \n","            if (batch_idx + 1) % 10 == 0:\n","                print('[{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\tClass Loss: {:.6f}\\tDomain Loss: {:.6f}'.format(\n","                    batch_idx * len(input2), len(F_train_data),\n","                    100. * batch_idx / len(source_data2), loss.item(), class_loss.item(),\n","                    domain_loss.item()\n","                ))\n","                \n","            total_loss.append(loss.item())\n","            c_loss.append( class_loss.item())\n","            d_loss.append(domain_loss.item())\n","   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ibv8w5tsEy6N"},"source":["def test(extractor, class_classifier, domain_classifier, source_data1, source_data2, target_data):\n"," \n","    # setup the network\n","    extractor.eval()\n","    class_classifier.eval()\n","    domain_classifier.eval()\n","    source_correct1 = 0.0\n","    source_correct2 = 0.0\n","    target_correct = 0.0\n","    domain_correct = 0.0\n","    tgt_correct = 0.0\n","    src_correct1 = 0.0\n","    src_correct2 = 0.0\n","\n","    for batch_idx, sdata in enumerate(source_data1):\n","        # setup hyperparameters\n","        p = float(batch_idx) / len(source_data1)\n","        constant = 2. / (1. + np.exp(-10 * p)) - 1.\n","\n","        input1, label1, domain1 = sdata.text, sdata.label, sdata.domain\n","        \n","        output1 = class_classifier(extractor(input1)).squeeze(1)\n","        output1 = torch.round(torch.sigmoid(output1))\n","        source_correct1 += (output1 == label1).cpu().sum()\n","\n","        src_preds = domain_classifier(extractor(input1), constant).squeeze(1)\n","        src_preds = torch.round(torch.sigmoid(src_preds))\n","        src_correct1 += (src_preds == domain1).cpu().sum()\n","\n","    for batch_idx, sdata in enumerate(source_data2):\n","        # setup hyperparameters\n","        p = float(batch_idx) / len(source_data2)\n","        constant = 2. / (1. + np.exp(-10 * p)) - 1.\n","\n","        input2, label2, domain2 = sdata.text, sdata.label, sdata.domain\n","       \n","        output2 = class_classifier(extractor(input2)).squeeze(1)\n","        output2 = torch.round(torch.sigmoid(output2))\n","        source_correct2 += (output2 == label2).cpu().sum()\n","\n","        src_preds = domain_classifier(extractor(input2), constant).squeeze(1)\n","        src_preds = torch.round(torch.sigmoid(src_preds))\n","        src_correct2 += (src_preds == domain2).cpu().sum()\n","\n","    for batch_idx, tdata in enumerate(target_data):\n","        # setup hyperparameters\n","        p = float(batch_idx) / len(target_data)\n","        constant = 2. / (1. + np.exp(-10 * p)) - 1\n","\n","        input3, label3, domain3 = tdata.text, tdata.label, tdata.domain\n","        \n","        output3 = class_classifier(extractor(input3)).squeeze(1)\n","        output3 = torch.round(torch.sigmoid(output3))\n","        target_correct += (output3 == label3).cpu().sum()\n","\n","        src_preds = domain_classifier(extractor(input3), constant).squeeze(1)\n","        src_preds = torch.round(torch.sigmoid(src_preds))\n","        tgt_correct += (src_preds == domain3).cpu().sum()\n","\n","\n","    domain_correct = tgt_correct + src_correct1 + src_correct2\n","\n","    print('\\nSource1 Accuracy(classification): {}/{} ({:.4f}%)\\nSource2 Accuracy(classification): {}/{} ({:.4f}%)\\nTarget Accuracy(classification): {}/{} ({:.4f}%)\\n'\n","          'Domain Accuracy: {}/{} ({:.4f}%)\\n'.\n","        format(\n","        source_correct1, len(T_test_data), 100. * float(source_correct1) / len(T_test_data),\n","        source_correct2, len(F_test_data), 100. * float(source_correct2) / len(F_test_data),\n","        target_correct, len(W_test_data), 100. * float(target_correct) / len(W_test_data),\n","        domain_correct, len(T_test_data) + len(F_test_data) + len(W_test_data),\n","        100. * float(domain_correct) / (len(T_test_data) + len(F_test_data) \n","                                        + len(W_test_data))\n","    ))\n","    acc_list1.append(100. * float(source_correct1) / len(T_test_data))\n","    acc_list2.append(100. * float(source_correct2) / len(F_test_data))\n","    acc_list3.append(100. * float(target_correct) / len(W_test_data))\n","    acc_list4.append(100. * float(domain_correct) / (len(T_test_data) + len(F_test_data) \n","                                        + len(W_test_data)))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFeWfX_oEy6O"},"source":["def main():\n","\n","    # prepare the source data and target data\n","    src_train_data1 = W_train_iterator\n","    src_test_data1 = W_test_iterator\n","    src_train_data2 = F_train_iterator\n","    src_test_data2 = F_test_iterator\n","    \n","    tgt_train_data = T_train_iterator\n","    tgt_test_data = T_test_iterator\n","\n","    optimizer = optim.SGD([\n","            {'params': extractor.parameters()},\n","                            {'params': class_classifier.parameters()},\n","                            {'params': domain_classifier.parameters()}\n","    ], lr= 0.01, momentum= 0.9)\n","\n","\n","    for epoch in range(100):\n","    \n","        print('Epoch: {}'.format(epoch))\n","        train('dann', extractor, class_classifier, domain_classifier, criterion,\n","                    src_train_data1, src_train_data2, tgt_train_data, optimizer, epoch)\n","        test(extractor, class_classifier, domain_classifier, src_test_data1, src_test_data2, tgt_test_data)\n","        \n","total_loss, d_loss, c_loss = [],[],[]\n","acc_list1, acc_list2, acc_list3, acc_list4 = [],[],[],[]\n","if __name__ == '__main__':\n","    gamma = 10\n","    theta = 1\n","    time_start=time.time()\n","    main()\n","    time_end=time.time()\n","    print('total run time: (min)',(time_end-time_start)/60.)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BgR1TDPPEy6P"},"source":["plt.plot(range(len(total_loss)),total_loss,c='r',label='total_loss')\n","plt.plot(d_loss,c='b',label='domain_loss')\n","plt.plot(c_loss,c='y',label='clf_loss')\n","plt.title('traing')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SuPpcHlVEy6Q"},"source":["print('max target accuracy: ',max(acc_list3))\n","plt.plot(range(len(acc_list1)),acc_list1,c='r',label='source1(wiki)_acc')\n","plt.plot(acc_list2,c='b',label='source2(forum)_acc')\n","plt.plot(acc_list3,c='g',label='target(twitter)_acc')\n","plt.plot(acc_list4,c='y',label='domain_acc')\n","plt.axhline(max(acc_list3),c='b',linestyle='--')\n","plt.title('testing')\n","plt.legend(loc='best')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eS1nBHCHG4dc"},"source":["100 epoches, 100, 52.25\n","100 epoches, 200, 56.55\n","100, 400, 73.85\n","600, 75.0\n","800, 74.3\n","1000,  75.3\n","2000, 77.65"]},{"cell_type":"markdown","metadata":{"id":"XAva4E2jQ4_C"},"source":["200 epoches, 6000, 75.8\n","100 epoches, 4000, 77.4\n","100 epoches, 2000, 77.65\n"]}]}